{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e620f91-1fe4-49f1-b0e4-2182c6fb7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "import os\n",
    "sep = os.sep\n",
    "# Добавить в путь до родительской папки\\n\",\n",
    "#sys.path.append(os.path.join(sys.path[0], f'..{sep}'))\n",
    "#sys.path.append(os.path.join(os.getcwd(), f'..{sep}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8e2586-65b4-4f53-a8d1-48e0c9bfa1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "#\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4633e79-0d6d-492b-87a6-15a3e43b5e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e61fdc-9aad-4452-89e2-520ae43a1f31",
   "metadata": {},
   "source": [
    "# 1 Introduction to recurrent neural networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdc15ae-b724-485a-a920-6f1635885935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630cd14-54e8-43f2-a1bc-2b19ff04065e",
   "metadata": {},
   "source": [
    "# 2 Fundamentals of Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b25e3d0-be13-4e20-ac46-1e4f3d80ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 't', ' ', 'i', 's', ' ', 'u', 'n', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'y', ' ', 'g', 'o', 'o', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "# Токенизация символов\n",
    "text = 'It is unbelievably good!'\n",
    "\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b93a741-1458-4474-8160-ffa6f0ac6415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'i', ',', ' ', 't', 'h', 'e', 'r', 'e', '!']\n"
     ]
    }
   ],
   "source": [
    "text = 'Hi, there!'\n",
    "\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051c33b0-4747-4a68-a0f3-1ad9e23e38f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'unbelievably', 'good', '!']\n"
     ]
    }
   ],
   "source": [
    "# Токенизация слов\n",
    "text = 'It is unbelievably good!'\n",
    "text = text.replace('!', ' !')\n",
    "\n",
    "tokens = text.split(' ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3c772d-1935-4bfc-82ba-61fbafa1972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'there', '!']\n"
     ]
    }
   ],
   "source": [
    "text = 'Hi, there!'\n",
    "for x in list(',!'):\n",
    "    text = text.replace(f'{x}', f' {x}')\n",
    "\n",
    "tokens = text.split(' ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f3610-448e-4867-a010-cbc7ac2b58bb",
   "metadata": {},
   "source": [
    "# 3 Prepare data to train the LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4efb6-0e2b-4e9c-8620-73ce4be34230",
   "metadata": {},
   "source": [
    "Загрузить текстовый файл Анны Карениной по ссылке https://github.com/LeanManager/NLP-PyTorch/tree/master/data.\n",
    "Сохраните его как *anna.txt*. Открыть файл и удалить все после строки 39888, \"END OF THIS PROJECT GUTENBERG EBOOK ANNA KARENINA\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f54cc-76ed-480a-a315-62f205c9cc2d",
   "metadata": {},
   "source": [
    "## 3.1 Download the clean up the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5646da1-6da8-493a-9118-7a9e6d63254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n"
     ]
    }
   ],
   "source": [
    "# Роман (на английском) \"Анна Каренина\"\n",
    "with open(f'..{sep}data{sep}ch08{sep}anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "words = text.split(' ') \n",
    "print( words[0:20] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe98c71-84f3-4476-b00b-c3007648bbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 1966150)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4658fc69-e100-4a5d-a663-524e947aa137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_', '8', 'y', 'k', ')', 'u', '9', '7', 'c', ':', 'm', '3', 'g', 'r', 'f', 's', 'i', '?', 'z', '6', \"'\", '!', 'p', ' ', 't', 'b', '\"', 'o', 'j', 'q', '5', '4', '1', 'x', '0', 'e', '.', 'h', 'v', 'd', '\\n', ',', 'w', '`', 'l', '2', 'a', 'n', '-', ';', '('}\n"
     ]
    }
   ],
   "source": [
    "print(set(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f3aee55-622b-403e-8952-947c8ccbe1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистить текст\n",
    "clean_text = text.lower().replace('\\n', ' ')\n",
    "clean_text = clean_text.replace('-', ' ')\n",
    "\n",
    "# Перед и после символов <,.:;?!$()/_&%*@'`> вставить пробелы\n",
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text = clean_text.replace(f'{x}', f' {x} ')\n",
    "\n",
    "clean_text = clean_text.replace('\"', ' \" ') \n",
    "\n",
    "# Переводим в спсисок слов (токенов)\n",
    "text = clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74bb80e2-f093-4fb0-bfb1-02424fe49661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 437098)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fd390c-0733-40d0-8233-12df35c159aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(text)\n",
    "#word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283c0eae-dda4-4f9c-b7cc-90cd0bfabb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
     ]
    }
   ],
   "source": [
    "# Получить уникальные слова (токены), отсортированные\n",
    "# по частоте встречаемости в тексте\n",
    "words = sorted(word_counts, key=word_counts.get, reverse=True) \n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413add85-d188-46f2-8916-98c6ce0cb708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text contains 437098 words\n",
      "There are 12778 unique tokens\n"
     ]
    }
   ],
   "source": [
    "text_length = len(text)\n",
    "num_unique_words = len(words)\n",
    "print(f'Text contains {text_length} words')\n",
    "print(f'There are {num_unique_words} unique tokens')  \n",
    "\n",
    "# Слово (токен) является ключом словаря\n",
    "word_to_int = {v: k for k, v in enumerate(words)} \n",
    "# Число является ключом словаря (токенов)\n",
    "int_to_word = {k: v for k, v in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3215163a-23df-4ef7-a53a-9d1ba4b7e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7, \"'\": 8, 'a': 9}\n"
     ]
    }
   ],
   "source": [
    "print({k: v for k, v in word_to_int.items() if k in words[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91a54c00-60e2-4548-a566-7c7356cd1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he', 8: \"'\", 9: 'a'}\n"
     ]
    }
   ],
   "source": [
    "print({k: v for k, v in int_to_word.items() if v in words[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a550375a-c36b-4fc2-95c3-c5fba385fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
      "[208, 2755, 280, 2981, 83, 31, 2419, 35, 202, 685, 362, 38, 685, 10, 236, 147, 166, 1, 149, 12]\n"
     ]
    }
   ],
   "source": [
    "# Пример, как узнать номер слова (токена) по словарю\n",
    "print(text[0:20])\n",
    "print([word_to_int[w] for w in text[0:20]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17de857f-3fd3-49b1-b9bf-23f725169c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(word_to_int['anna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e0e912-3728-4dcb-8f60-8d5015d2094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437098"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Перевести весь текст в список индексов\n",
    "word_ids = [word_to_int[w] for w in text]  \n",
    "#print(word_ids[0:20])\n",
    "len(word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f081f5-ffd0-4c99-9afe-a21ddd990568",
   "metadata": {},
   "source": [
    "## 3.2 Create batches of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5edfdf-9c71-47cb-bebc-e067ab83a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Длина последовательности (100 выбрали произвольно)\n",
    "seq_len = 100\n",
    "xys = []\n",
    "\n",
    "# Создать пары последовательностей (x, y) для обучения.\n",
    "# Каждый x это последовательность со 100 индексами.\n",
    "# Затем сдвигаем окно вправо на один токен\n",
    "# и используем его в качестве целевого y.\n",
    "for n in range(0, len(word_ids)-seq_len-1):\n",
    "    x = word_ids[n : n+seq_len]\n",
    "    y = word_ids[n+1 : n+seq_len+1]\n",
    "    xys.append((torch.tensor(x), (torch.tensor(y))))\n",
    "\n",
    "# Сдвигая последовательность на один токен вправо и используя его в качестве выходных данных,\n",
    "# модель обучается предсказывать следующий токен, учитывая предыдущие токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12254e98-a588-470a-91a6-861766ce5406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436997"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1c1033-c26f-47cc-890a-b3904a37eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eee82f33-b437-418a-af14-1e1982f879bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  39,   31,    2,  ...,  688,  142,    7],\n",
      "        [ 156, 5293,    0,  ...,   38,  330,    0],\n",
      "        [   3,   97,    0,  ...,    0, 1774,   34],\n",
      "        ...,\n",
      "        [  16,  156,    9,  ...,  113,    5,  533],\n",
      "        [   3,    4,   31,  ...,   98,    5,   98],\n",
      "        [ 289,   19,   23,  ...,    9,  828,  550]])\n",
      "tensor([[  31,    2, 2727,  ...,  142,    7,    0],\n",
      "        [5293,    0,   16,  ...,  330,    0,    3],\n",
      "        [  97,    0,    4,  ..., 1774,   34,    3],\n",
      "        ...,\n",
      "        [ 156,    9,  489,  ...,    5,  533,   27],\n",
      "        [   4,   31,   25,  ...,    5,   98,    1],\n",
      "        [  19,   23,    1,  ...,  828,  550,    1]])\n",
      "torch.Size([32, 100]) torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Пакеты данных для обучения, по 32 пары (x, y) в каждом пакете.\n",
    "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "print(x); print(y);\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a669818-3ce5-456e-8e00-c1086106a15b",
   "metadata": {},
   "source": [
    "# 4 Building and Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fe2529c-9922-49d6-b22d-736ea892675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df697e77-a465-4b27-8411-0a82a151a00c",
   "metadata": {},
   "source": [
    "## 4.1 Build an LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd83d0b-b4ab-4f43-b51f-348206befd35",
   "metadata": {},
   "source": [
    "**Класс torch.nn.Embedding()** используется для создания embedding-слоя в нейронной сети. Embedding-слой это обучаемая таблица поиска, которая сопоставляет целочисленные индексы с плотными векторными представлениями (embedding).\n",
    "\n",
    "При создании экземпляра torch.nn.Embedding() необходимо указать два основных параметра: num_embeddings, размер словаря (общее количество уникальных токенов), и embedding_dim, размер каждого embedding-вектора.\n",
    "\n",
    "Внутри класс создает матрицу (или таблицу поиска) формы (num_embeddings, embedding_dim), где каждая строка соответствует embedding-вектору для конкретного индекса. Изначально эти embeddings инициализируются случайным образом, но изучаются и обновляются во время обучения с помощью backpropagation.\n",
    "\n",
    "Когда передается тензор индексов в embedding-слой (во время прямого прохода сети), он ищет соответствующие embedding-векторы в таблице поиска и возвращает их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41aea6ac-cfe0-4484-a008-a2f244407608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс имеет три слоя:\n",
    "# слой embedding, слой LSTM и последний линейный слой.\n",
    "class WordLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # input_size в слое nn.LSTM\n",
    "                 input_size=128,\n",
    "                 # Длина embedding (вектора) для кодирование одного токена\n",
    "                 n_embed=128,\n",
    "                 # num_layers в слое nn.LSTM\n",
    "                 n_layers=3,\n",
    "                 # dropout в слое nn.LSTM\n",
    "                 drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_embed = n_embed\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Кол-во уникальных токенов\n",
    "        vocab_size = len(word_to_int)\n",
    "        # Тренировочные данные сначала проходят через embedding-слой\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embed)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.n_embed,\n",
    "            num_layers=self.n_layers,\n",
    "            dropout=self.drop_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # На выходе получаем logits - по одному числу для каждого уникального токена\n",
    "        self.fc = nn.Linear(input_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        embed = self.embedding(x)\n",
    "        # На каждом шаге входными данными являются текущий токен и предыдущее скрытое состояние,\n",
    "        # а выходными данными - следующий токен и следующее скрытое состояние\n",
    "        x, hc = self.lstm(embed, hc)\n",
    "        x = self.fc(x)\n",
    "        return x, hc        \n",
    "\n",
    "    # Инициализация весов нулями для первого токена\n",
    "    # во входной последовательности\n",
    "    # n_seqs: batch_size\n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        return (\n",
    "            weight.new(self.n_layers, n_seqs, self.n_embed).zero_(),\n",
    "            weight.new(self.n_layers, n_seqs, self.n_embed).zero_()\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14f75fa7-deed-40c2-9e4e-e316c0d660ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordLSTM(\n",
       "  (embedding): Embedding(12778, 128)\n",
       "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=12778, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WordLSTM().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fea74f3e-3291-4e0a-9bff-826dd7bce9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# Не забываем, что внутриCrossEntropyLoss() уже зашита функция Softmax\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d9cd0-4ccf-4dc1-be70-5ebd47ae74a0",
   "metadata": {},
   "source": [
    "## 4.2 Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23b43312-bd2b-4c56-bcac-a803d4c4ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iteration 1000, average loss = 6.391811\n",
      "Epoch 1, iteration 2000, average loss = 6.216393\n",
      "Epoch 1, iteration 3000, average loss = 6.157208\n",
      "Epoch 1, iteration 4000, average loss = 6.099291\n",
      "Epoch 1, iteration 5000, average loss = 6.003661\n",
      "Epoch 1, iteration 6000, average loss = 5.897637\n",
      "Epoch 1, iteration 7000, average loss = 5.796952\n",
      "Epoch 1, iteration 8000, average loss = 5.705112\n",
      "Epoch 1, iteration 9000, average loss = 5.621706\n",
      "Epoch 1, iteration 10000, average loss = 5.546037\n",
      "Epoch 1, iteration 11000, average loss = 5.476927\n",
      "Epoch 1, iteration 12000, average loss = 5.414001\n",
      "Epoch 1, iteration 13000, average loss = 5.355855\n",
      "Epoch 2, iteration 1000, average loss = 4.575553\n",
      "Epoch 2, iteration 2000, average loss = 4.551744\n",
      "Epoch 2, iteration 3000, average loss = 4.530076\n",
      "Epoch 2, iteration 4000, average loss = 4.508861\n",
      "Epoch 2, iteration 5000, average loss = 4.489376\n",
      "Epoch 2, iteration 6000, average loss = 4.471129\n",
      "Epoch 2, iteration 7000, average loss = 4.453864\n",
      "Epoch 2, iteration 8000, average loss = 4.437845\n",
      "Epoch 2, iteration 9000, average loss = 4.421810\n",
      "Epoch 2, iteration 10000, average loss = 4.406173\n",
      "Epoch 2, iteration 11000, average loss = 4.391288\n",
      "Epoch 2, iteration 12000, average loss = 4.377158\n",
      "Epoch 2, iteration 13000, average loss = 4.363394\n",
      "Epoch 3, iteration 1000, average loss = 4.159147\n",
      "Epoch 3, iteration 2000, average loss = 4.149089\n",
      "Epoch 3, iteration 3000, average loss = 4.138774\n",
      "Epoch 3, iteration 4000, average loss = 4.129710\n",
      "Epoch 3, iteration 5000, average loss = 4.120112\n",
      "Epoch 3, iteration 6000, average loss = 4.111252\n",
      "Epoch 3, iteration 7000, average loss = 4.102898\n",
      "Epoch 3, iteration 8000, average loss = 4.093772\n",
      "Epoch 3, iteration 9000, average loss = 4.084973\n",
      "Epoch 3, iteration 10000, average loss = 4.076418\n",
      "Epoch 3, iteration 11000, average loss = 4.067644\n",
      "Epoch 3, iteration 12000, average loss = 4.059089\n",
      "Epoch 3, iteration 13000, average loss = 4.050575\n",
      "Epoch 4, iteration 1000, average loss = 3.923327\n",
      "Epoch 4, iteration 2000, average loss = 3.917958\n",
      "Epoch 4, iteration 3000, average loss = 3.912618\n",
      "Epoch 4, iteration 4000, average loss = 3.905123\n",
      "Epoch 4, iteration 5000, average loss = 3.898568\n",
      "Epoch 4, iteration 6000, average loss = 3.892492\n",
      "Epoch 4, iteration 7000, average loss = 3.885787\n",
      "Epoch 4, iteration 8000, average loss = 3.879026\n",
      "Epoch 4, iteration 9000, average loss = 3.872838\n",
      "Epoch 4, iteration 10000, average loss = 3.866774\n",
      "Epoch 4, iteration 11000, average loss = 3.860785\n",
      "Epoch 4, iteration 12000, average loss = 3.854603\n",
      "Epoch 4, iteration 13000, average loss = 3.848888\n",
      "Epoch 5, iteration 1000, average loss = 3.755557\n",
      "Epoch 5, iteration 2000, average loss = 3.750313\n",
      "Epoch 5, iteration 3000, average loss = 3.745554\n",
      "Epoch 5, iteration 4000, average loss = 3.740235\n",
      "Epoch 5, iteration 5000, average loss = 3.735418\n",
      "Epoch 5, iteration 6000, average loss = 3.730133\n",
      "Epoch 5, iteration 7000, average loss = 3.725059\n",
      "Epoch 5, iteration 8000, average loss = 3.719809\n",
      "Epoch 5, iteration 9000, average loss = 3.714797\n",
      "Epoch 5, iteration 10000, average loss = 3.709846\n",
      "Epoch 5, iteration 11000, average loss = 3.704618\n",
      "Epoch 5, iteration 12000, average loss = 3.699638\n",
      "Epoch 5, iteration 13000, average loss = 3.694922\n",
      "Epoch 6, iteration 1000, average loss = 3.620595\n",
      "Epoch 6, iteration 2000, average loss = 3.615748\n",
      "Epoch 6, iteration 3000, average loss = 3.612583\n",
      "Epoch 6, iteration 4000, average loss = 3.608297\n",
      "Epoch 6, iteration 5000, average loss = 3.604042\n",
      "Epoch 6, iteration 6000, average loss = 3.599746\n",
      "Epoch 6, iteration 7000, average loss = 3.595321\n",
      "Epoch 6, iteration 8000, average loss = 3.591334\n",
      "Epoch 6, iteration 9000, average loss = 3.586858\n",
      "Epoch 6, iteration 10000, average loss = 3.582766\n",
      "Epoch 6, iteration 11000, average loss = 3.578590\n",
      "Epoch 6, iteration 12000, average loss = 3.574381\n",
      "Epoch 6, iteration 13000, average loss = 3.570299\n",
      "Epoch 7, iteration 1000, average loss = 3.510841\n",
      "Epoch 7, iteration 2000, average loss = 3.505511\n",
      "Epoch 7, iteration 3000, average loss = 3.501517\n",
      "Epoch 7, iteration 4000, average loss = 3.497832\n",
      "Epoch 7, iteration 5000, average loss = 3.494156\n",
      "Epoch 7, iteration 6000, average loss = 3.490480\n",
      "Epoch 7, iteration 7000, average loss = 3.486884\n",
      "Epoch 7, iteration 8000, average loss = 3.483245\n",
      "Epoch 7, iteration 9000, average loss = 3.479590\n",
      "Epoch 7, iteration 10000, average loss = 3.476317\n",
      "Epoch 7, iteration 11000, average loss = 3.472677\n",
      "Epoch 7, iteration 12000, average loss = 3.469178\n",
      "Epoch 7, iteration 13000, average loss = 3.465950\n",
      "Epoch 8, iteration 1000, average loss = 3.415122\n",
      "Epoch 8, iteration 2000, average loss = 3.413000\n",
      "Epoch 8, iteration 3000, average loss = 3.409229\n",
      "Epoch 8, iteration 4000, average loss = 3.406450\n",
      "Epoch 8, iteration 5000, average loss = 3.402654\n",
      "Epoch 8, iteration 6000, average loss = 3.399194\n",
      "Epoch 8, iteration 7000, average loss = 3.395758\n",
      "Epoch 8, iteration 8000, average loss = 3.392704\n",
      "Epoch 8, iteration 9000, average loss = 3.389556\n",
      "Epoch 8, iteration 10000, average loss = 3.386517\n",
      "Epoch 8, iteration 11000, average loss = 3.383480\n",
      "Epoch 8, iteration 12000, average loss = 3.380493\n",
      "Epoch 8, iteration 13000, average loss = 3.377452\n",
      "Epoch 9, iteration 1000, average loss = 3.329557\n",
      "Epoch 9, iteration 2000, average loss = 3.326801\n",
      "Epoch 9, iteration 3000, average loss = 3.324378\n",
      "Epoch 9, iteration 4000, average loss = 3.322474\n",
      "Epoch 9, iteration 5000, average loss = 3.319906\n",
      "Epoch 9, iteration 6000, average loss = 3.317311\n",
      "Epoch 9, iteration 7000, average loss = 3.315157\n",
      "Epoch 9, iteration 8000, average loss = 3.312586\n",
      "Epoch 9, iteration 9000, average loss = 3.310138\n",
      "Epoch 9, iteration 10000, average loss = 3.307650\n",
      "Epoch 9, iteration 11000, average loss = 3.305026\n",
      "Epoch 9, iteration 12000, average loss = 3.302269\n",
      "Epoch 9, iteration 13000, average loss = 3.299865\n",
      "Epoch 10, iteration 1000, average loss = 3.259297\n",
      "Epoch 10, iteration 2000, average loss = 3.257015\n",
      "Epoch 10, iteration 3000, average loss = 3.254341\n",
      "Epoch 10, iteration 4000, average loss = 3.252252\n",
      "Epoch 10, iteration 5000, average loss = 3.250060\n",
      "Epoch 10, iteration 6000, average loss = 3.247894\n",
      "Epoch 10, iteration 7000, average loss = 3.245755\n",
      "Epoch 10, iteration 8000, average loss = 3.243221\n",
      "Epoch 10, iteration 9000, average loss = 3.240489\n",
      "Epoch 10, iteration 10000, average loss = 3.238273\n",
      "Epoch 10, iteration 11000, average loss = 3.235977\n",
      "Epoch 10, iteration 12000, average loss = 3.233949\n",
      "Epoch 10, iteration 13000, average loss = 3.231622\n",
      "Epoch 11, iteration 1000, average loss = 3.197337\n",
      "Epoch 11, iteration 2000, average loss = 3.194705\n",
      "Epoch 11, iteration 3000, average loss = 3.191820\n",
      "Epoch 11, iteration 4000, average loss = 3.189301\n",
      "Epoch 11, iteration 5000, average loss = 3.187603\n",
      "Epoch 11, iteration 6000, average loss = 3.185487\n",
      "Epoch 11, iteration 7000, average loss = 3.182553\n",
      "Epoch 11, iteration 8000, average loss = 3.180779\n",
      "Epoch 11, iteration 9000, average loss = 3.178713\n",
      "Epoch 11, iteration 10000, average loss = 3.176774\n",
      "Epoch 11, iteration 11000, average loss = 3.174843\n",
      "Epoch 11, iteration 12000, average loss = 3.173035\n",
      "Epoch 11, iteration 13000, average loss = 3.171151\n",
      "Epoch 12, iteration 1000, average loss = 3.138214\n",
      "Epoch 12, iteration 2000, average loss = 3.137314\n",
      "Epoch 12, iteration 3000, average loss = 3.135681\n",
      "Epoch 12, iteration 4000, average loss = 3.134149\n",
      "Epoch 12, iteration 5000, average loss = 3.132789\n",
      "Epoch 12, iteration 6000, average loss = 3.130907\n",
      "Epoch 12, iteration 7000, average loss = 3.128550\n",
      "Epoch 12, iteration 8000, average loss = 3.127029\n",
      "Epoch 12, iteration 9000, average loss = 3.124920\n",
      "Epoch 12, iteration 10000, average loss = 3.123063\n",
      "Epoch 12, iteration 11000, average loss = 3.121182\n",
      "Epoch 12, iteration 12000, average loss = 3.119432\n",
      "Epoch 12, iteration 13000, average loss = 3.117667\n",
      "Epoch 13, iteration 1000, average loss = 3.089092\n",
      "Epoch 13, iteration 2000, average loss = 3.087995\n",
      "Epoch 13, iteration 3000, average loss = 3.086292\n",
      "Epoch 13, iteration 4000, average loss = 3.084268\n",
      "Epoch 13, iteration 5000, average loss = 3.082081\n",
      "Epoch 13, iteration 6000, average loss = 3.080504\n",
      "Epoch 13, iteration 7000, average loss = 3.078799\n",
      "Epoch 13, iteration 8000, average loss = 3.077268\n",
      "Epoch 13, iteration 9000, average loss = 3.075610\n",
      "Epoch 13, iteration 10000, average loss = 3.073811\n",
      "Epoch 13, iteration 11000, average loss = 3.071909\n",
      "Epoch 13, iteration 12000, average loss = 3.070075\n",
      "Epoch 13, iteration 13000, average loss = 3.068660\n",
      "Epoch 14, iteration 1000, average loss = 3.043203\n",
      "Epoch 14, iteration 2000, average loss = 3.039305\n",
      "Epoch 14, iteration 3000, average loss = 3.037690\n",
      "Epoch 14, iteration 4000, average loss = 3.036946\n",
      "Epoch 14, iteration 5000, average loss = 3.035715\n",
      "Epoch 14, iteration 6000, average loss = 3.034400\n",
      "Epoch 14, iteration 7000, average loss = 3.033313\n",
      "Epoch 14, iteration 8000, average loss = 3.031894\n",
      "Epoch 14, iteration 9000, average loss = 3.030459\n",
      "Epoch 14, iteration 10000, average loss = 3.029187\n",
      "Epoch 14, iteration 11000, average loss = 3.027426\n",
      "Epoch 14, iteration 12000, average loss = 3.026037\n",
      "Epoch 14, iteration 13000, average loss = 3.024476\n",
      "Epoch 15, iteration 1000, average loss = 3.002397\n",
      "Epoch 15, iteration 2000, average loss = 3.000183\n",
      "Epoch 15, iteration 3000, average loss = 2.997902\n",
      "Epoch 15, iteration 4000, average loss = 2.997238\n",
      "Epoch 15, iteration 5000, average loss = 2.995466\n",
      "Epoch 15, iteration 6000, average loss = 2.993873\n",
      "Epoch 15, iteration 7000, average loss = 2.992328\n",
      "Epoch 15, iteration 8000, average loss = 2.990641\n",
      "Epoch 15, iteration 9000, average loss = 2.989524\n",
      "Epoch 15, iteration 10000, average loss = 2.988283\n",
      "Epoch 15, iteration 11000, average loss = 2.986920\n",
      "Epoch 15, iteration 12000, average loss = 2.985613\n",
      "Epoch 15, iteration 13000, average loss = 2.984235\n",
      "Epoch 16, iteration 1000, average loss = 2.960473\n",
      "Epoch 16, iteration 2000, average loss = 2.960311\n",
      "Epoch 16, iteration 3000, average loss = 2.959683\n",
      "Epoch 16, iteration 4000, average loss = 2.958103\n",
      "Epoch 16, iteration 5000, average loss = 2.956875\n",
      "Epoch 16, iteration 6000, average loss = 2.956043\n",
      "Epoch 16, iteration 7000, average loss = 2.954677\n",
      "Epoch 16, iteration 8000, average loss = 2.953389\n",
      "Epoch 16, iteration 9000, average loss = 2.951878\n",
      "Epoch 16, iteration 10000, average loss = 2.950546\n",
      "Epoch 16, iteration 11000, average loss = 2.949097\n",
      "Epoch 16, iteration 12000, average loss = 2.948005\n",
      "Epoch 16, iteration 13000, average loss = 2.946855\n",
      "Epoch 17, iteration 1000, average loss = 2.925872\n",
      "Epoch 17, iteration 2000, average loss = 2.924449\n",
      "Epoch 17, iteration 3000, average loss = 2.923533\n",
      "Epoch 17, iteration 4000, average loss = 2.922653\n",
      "Epoch 17, iteration 5000, average loss = 2.921725\n",
      "Epoch 17, iteration 6000, average loss = 2.920661\n",
      "Epoch 17, iteration 7000, average loss = 2.919472\n",
      "Epoch 17, iteration 8000, average loss = 2.918336\n",
      "Epoch 17, iteration 9000, average loss = 2.917272\n",
      "Epoch 17, iteration 10000, average loss = 2.916111\n",
      "Epoch 17, iteration 11000, average loss = 2.914887\n",
      "Epoch 17, iteration 12000, average loss = 2.913708\n",
      "Epoch 17, iteration 13000, average loss = 2.912498\n",
      "Epoch 18, iteration 1000, average loss = 2.891867\n",
      "Epoch 18, iteration 2000, average loss = 2.892280\n",
      "Epoch 18, iteration 3000, average loss = 2.891602\n",
      "Epoch 18, iteration 4000, average loss = 2.890048\n",
      "Epoch 18, iteration 5000, average loss = 2.889311\n",
      "Epoch 18, iteration 6000, average loss = 2.888106\n",
      "Epoch 18, iteration 7000, average loss = 2.887371\n",
      "Epoch 18, iteration 8000, average loss = 2.886234\n",
      "Epoch 18, iteration 9000, average loss = 2.885528\n",
      "Epoch 18, iteration 10000, average loss = 2.884289\n",
      "Epoch 18, iteration 11000, average loss = 2.883158\n",
      "Epoch 18, iteration 12000, average loss = 2.882048\n",
      "Epoch 18, iteration 13000, average loss = 2.881028\n",
      "Epoch 19, iteration 1000, average loss = 2.863205\n",
      "Epoch 19, iteration 2000, average loss = 2.861150\n",
      "Epoch 19, iteration 3000, average loss = 2.859732\n",
      "Epoch 19, iteration 4000, average loss = 2.859265\n",
      "Epoch 19, iteration 5000, average loss = 2.858205\n",
      "Epoch 19, iteration 6000, average loss = 2.857519\n",
      "Epoch 19, iteration 7000, average loss = 2.856741\n",
      "Epoch 19, iteration 8000, average loss = 2.855829\n",
      "Epoch 19, iteration 9000, average loss = 2.855058\n",
      "Epoch 19, iteration 10000, average loss = 2.853935\n",
      "Epoch 19, iteration 11000, average loss = 2.853023\n",
      "Epoch 19, iteration 12000, average loss = 2.852135\n",
      "Epoch 19, iteration 13000, average loss = 2.851272\n",
      "Epoch 20, iteration 1000, average loss = 2.834220\n",
      "Epoch 20, iteration 2000, average loss = 2.833941\n",
      "Epoch 20, iteration 3000, average loss = 2.832674\n",
      "Epoch 20, iteration 4000, average loss = 2.832064\n",
      "Epoch 20, iteration 5000, average loss = 2.830954\n",
      "Epoch 20, iteration 6000, average loss = 2.830164\n",
      "Epoch 20, iteration 7000, average loss = 2.829206\n",
      "Epoch 20, iteration 8000, average loss = 2.828449\n",
      "Epoch 20, iteration 9000, average loss = 2.827645\n",
      "Epoch 20, iteration 10000, average loss = 2.826398\n",
      "Epoch 20, iteration 11000, average loss = 2.825620\n",
      "Epoch 20, iteration 12000, average loss = 2.824439\n",
      "Epoch 20, iteration 13000, average loss = 2.823433\n",
      "Epoch 21, iteration 1000, average loss = 2.807578\n",
      "Epoch 21, iteration 2000, average loss = 2.806872\n",
      "Epoch 21, iteration 3000, average loss = 2.806150\n",
      "Epoch 21, iteration 4000, average loss = 2.805874\n",
      "Epoch 21, iteration 5000, average loss = 2.804771\n",
      "Epoch 21, iteration 6000, average loss = 2.804119\n",
      "Epoch 21, iteration 7000, average loss = 2.803522\n",
      "Epoch 21, iteration 8000, average loss = 2.802562\n",
      "Epoch 21, iteration 9000, average loss = 2.801682\n",
      "Epoch 21, iteration 10000, average loss = 2.800843\n",
      "Epoch 21, iteration 11000, average loss = 2.799681\n",
      "Epoch 21, iteration 12000, average loss = 2.798756\n",
      "Epoch 21, iteration 13000, average loss = 2.798016\n",
      "Epoch 22, iteration 1000, average loss = 2.782972\n",
      "Epoch 22, iteration 2000, average loss = 2.782515\n",
      "Epoch 22, iteration 3000, average loss = 2.781792\n",
      "Epoch 22, iteration 4000, average loss = 2.781010\n",
      "Epoch 22, iteration 5000, average loss = 2.780428\n",
      "Epoch 22, iteration 6000, average loss = 2.779884\n",
      "Epoch 22, iteration 7000, average loss = 2.779161\n",
      "Epoch 22, iteration 8000, average loss = 2.778114\n",
      "Epoch 22, iteration 9000, average loss = 2.777164\n",
      "Epoch 22, iteration 10000, average loss = 2.776478\n",
      "Epoch 22, iteration 11000, average loss = 2.775534\n",
      "Epoch 22, iteration 12000, average loss = 2.774659\n",
      "Epoch 22, iteration 13000, average loss = 2.773743\n",
      "Epoch 23, iteration 1000, average loss = 2.761000\n",
      "Epoch 23, iteration 2000, average loss = 2.759528\n",
      "Epoch 23, iteration 3000, average loss = 2.759244\n",
      "Epoch 23, iteration 4000, average loss = 2.757853\n",
      "Epoch 23, iteration 5000, average loss = 2.757005\n",
      "Epoch 23, iteration 6000, average loss = 2.756388\n",
      "Epoch 23, iteration 7000, average loss = 2.755535\n",
      "Epoch 23, iteration 8000, average loss = 2.754733\n",
      "Epoch 23, iteration 9000, average loss = 2.754111\n",
      "Epoch 23, iteration 10000, average loss = 2.753216\n",
      "Epoch 23, iteration 11000, average loss = 2.752186\n",
      "Epoch 23, iteration 12000, average loss = 2.751473\n",
      "Epoch 23, iteration 13000, average loss = 2.750677\n",
      "Epoch 24, iteration 1000, average loss = 2.738069\n",
      "Epoch 24, iteration 2000, average loss = 2.736628\n",
      "Epoch 24, iteration 3000, average loss = 2.735355\n",
      "Epoch 24, iteration 4000, average loss = 2.734666\n",
      "Epoch 24, iteration 5000, average loss = 2.733923\n",
      "Epoch 24, iteration 6000, average loss = 2.733175\n",
      "Epoch 24, iteration 7000, average loss = 2.732621\n",
      "Epoch 24, iteration 8000, average loss = 2.732141\n",
      "Epoch 24, iteration 9000, average loss = 2.731704\n",
      "Epoch 24, iteration 10000, average loss = 2.730831\n",
      "Epoch 24, iteration 11000, average loss = 2.729933\n",
      "Epoch 24, iteration 12000, average loss = 2.729329\n",
      "Epoch 24, iteration 13000, average loss = 2.728636\n",
      "Epoch 25, iteration 1000, average loss = 2.717164\n",
      "Epoch 25, iteration 2000, average loss = 2.716234\n",
      "Epoch 25, iteration 3000, average loss = 2.714520\n",
      "Epoch 25, iteration 4000, average loss = 2.714561\n",
      "Epoch 25, iteration 5000, average loss = 2.714149\n",
      "Epoch 25, iteration 6000, average loss = 2.713345\n",
      "Epoch 25, iteration 7000, average loss = 2.712825\n",
      "Epoch 25, iteration 8000, average loss = 2.712127\n",
      "Epoch 25, iteration 9000, average loss = 2.711426\n",
      "Epoch 25, iteration 10000, average loss = 2.710544\n",
      "Epoch 25, iteration 11000, average loss = 2.710124\n",
      "Epoch 25, iteration 12000, average loss = 2.709384\n",
      "Epoch 25, iteration 13000, average loss = 2.708762\n",
      "Epoch 26, iteration 1000, average loss = 2.700336\n",
      "Epoch 26, iteration 2000, average loss = 2.697188\n",
      "Epoch 26, iteration 3000, average loss = 2.696049\n",
      "Epoch 26, iteration 4000, average loss = 2.695079\n",
      "Epoch 26, iteration 5000, average loss = 2.694415\n",
      "Epoch 26, iteration 6000, average loss = 2.694245\n",
      "Epoch 26, iteration 7000, average loss = 2.693669\n",
      "Epoch 26, iteration 8000, average loss = 2.693086\n",
      "Epoch 26, iteration 9000, average loss = 2.692095\n",
      "Epoch 26, iteration 10000, average loss = 2.691511\n",
      "Epoch 26, iteration 11000, average loss = 2.690889\n",
      "Epoch 26, iteration 12000, average loss = 2.690227\n",
      "Epoch 26, iteration 13000, average loss = 2.689604\n",
      "Epoch 27, iteration 1000, average loss = 2.679978\n",
      "Epoch 27, iteration 2000, average loss = 2.677659\n",
      "Epoch 27, iteration 3000, average loss = 2.677141\n",
      "Epoch 27, iteration 4000, average loss = 2.677000\n",
      "Epoch 27, iteration 5000, average loss = 2.676746\n",
      "Epoch 27, iteration 6000, average loss = 2.676320\n",
      "Epoch 27, iteration 7000, average loss = 2.675255\n",
      "Epoch 27, iteration 8000, average loss = 2.674646\n",
      "Epoch 27, iteration 9000, average loss = 2.673959\n",
      "Epoch 27, iteration 10000, average loss = 2.673374\n",
      "Epoch 27, iteration 11000, average loss = 2.672662\n",
      "Epoch 27, iteration 12000, average loss = 2.671979\n",
      "Epoch 27, iteration 13000, average loss = 2.671438\n",
      "Epoch 28, iteration 1000, average loss = 2.660148\n",
      "Epoch 28, iteration 2000, average loss = 2.660130\n",
      "Epoch 28, iteration 3000, average loss = 2.659792\n",
      "Epoch 28, iteration 4000, average loss = 2.659002\n",
      "Epoch 28, iteration 5000, average loss = 2.658583\n",
      "Epoch 28, iteration 6000, average loss = 2.658251\n",
      "Epoch 28, iteration 7000, average loss = 2.657616\n",
      "Epoch 28, iteration 8000, average loss = 2.657168\n",
      "Epoch 28, iteration 9000, average loss = 2.656350\n",
      "Epoch 28, iteration 10000, average loss = 2.655724\n",
      "Epoch 28, iteration 11000, average loss = 2.655355\n",
      "Epoch 28, iteration 12000, average loss = 2.654843\n",
      "Epoch 28, iteration 13000, average loss = 2.654182\n",
      "Epoch 29, iteration 1000, average loss = 2.643522\n",
      "Epoch 29, iteration 2000, average loss = 2.642956\n",
      "Epoch 29, iteration 3000, average loss = 2.642742\n",
      "Epoch 29, iteration 4000, average loss = 2.642398\n",
      "Epoch 29, iteration 5000, average loss = 2.641565\n",
      "Epoch 29, iteration 6000, average loss = 2.640845\n",
      "Epoch 29, iteration 7000, average loss = 2.640399\n",
      "Epoch 29, iteration 8000, average loss = 2.639984\n",
      "Epoch 29, iteration 9000, average loss = 2.639372\n",
      "Epoch 29, iteration 10000, average loss = 2.639050\n",
      "Epoch 29, iteration 11000, average loss = 2.638548\n",
      "Epoch 29, iteration 12000, average loss = 2.638154\n",
      "Epoch 29, iteration 13000, average loss = 2.637752\n",
      "Epoch 30, iteration 1000, average loss = 2.627415\n",
      "Epoch 30, iteration 2000, average loss = 2.628173\n",
      "Epoch 30, iteration 3000, average loss = 2.626852\n",
      "Epoch 30, iteration 4000, average loss = 2.625865\n",
      "Epoch 30, iteration 5000, average loss = 2.625516\n",
      "Epoch 30, iteration 6000, average loss = 2.624897\n",
      "Epoch 30, iteration 7000, average loss = 2.624369\n",
      "Epoch 30, iteration 8000, average loss = 2.624015\n",
      "Epoch 30, iteration 9000, average loss = 2.623888\n",
      "Epoch 30, iteration 10000, average loss = 2.623460\n",
      "Epoch 30, iteration 11000, average loss = 2.622880\n",
      "Epoch 30, iteration 12000, average loss = 2.622558\n",
      "Epoch 30, iteration 13000, average loss = 2.621852\n",
      "Epoch 31, iteration 1000, average loss = 2.611148\n",
      "Epoch 31, iteration 2000, average loss = 2.611865\n",
      "Epoch 31, iteration 3000, average loss = 2.610825\n",
      "Epoch 31, iteration 4000, average loss = 2.610417\n",
      "Epoch 31, iteration 5000, average loss = 2.610236\n",
      "Epoch 31, iteration 6000, average loss = 2.609986\n",
      "Epoch 31, iteration 7000, average loss = 2.610167\n",
      "Epoch 31, iteration 8000, average loss = 2.609544\n",
      "Epoch 31, iteration 9000, average loss = 2.608734\n",
      "Epoch 31, iteration 10000, average loss = 2.608377\n",
      "Epoch 31, iteration 11000, average loss = 2.607939\n",
      "Epoch 31, iteration 12000, average loss = 2.607341\n",
      "Epoch 31, iteration 13000, average loss = 2.606890\n",
      "Epoch 32, iteration 1000, average loss = 2.597570\n",
      "Epoch 32, iteration 2000, average loss = 2.597403\n",
      "Epoch 32, iteration 3000, average loss = 2.597451\n",
      "Epoch 32, iteration 4000, average loss = 2.596729\n",
      "Epoch 32, iteration 5000, average loss = 2.596408\n",
      "Epoch 32, iteration 6000, average loss = 2.595658\n",
      "Epoch 32, iteration 7000, average loss = 2.595168\n",
      "Epoch 32, iteration 8000, average loss = 2.594783\n",
      "Epoch 32, iteration 9000, average loss = 2.594386\n",
      "Epoch 32, iteration 10000, average loss = 2.593873\n",
      "Epoch 32, iteration 11000, average loss = 2.593392\n",
      "Epoch 32, iteration 12000, average loss = 2.592965\n",
      "Epoch 32, iteration 13000, average loss = 2.592354\n",
      "Epoch 33, iteration 1000, average loss = 2.582099\n",
      "Epoch 33, iteration 2000, average loss = 2.581108\n",
      "Epoch 33, iteration 3000, average loss = 2.581711\n",
      "Epoch 33, iteration 4000, average loss = 2.582046\n",
      "Epoch 33, iteration 5000, average loss = 2.582440\n",
      "Epoch 33, iteration 6000, average loss = 2.582063\n",
      "Epoch 33, iteration 7000, average loss = 2.581679\n",
      "Epoch 33, iteration 8000, average loss = 2.581239\n",
      "Epoch 33, iteration 9000, average loss = 2.580732\n",
      "Epoch 33, iteration 10000, average loss = 2.580270\n",
      "Epoch 33, iteration 11000, average loss = 2.579717\n",
      "Epoch 33, iteration 12000, average loss = 2.579299\n",
      "Epoch 33, iteration 13000, average loss = 2.578885\n",
      "Epoch 34, iteration 1000, average loss = 2.568182\n",
      "Epoch 34, iteration 2000, average loss = 2.568574\n",
      "Epoch 34, iteration 3000, average loss = 2.568539\n",
      "Epoch 34, iteration 4000, average loss = 2.568650\n",
      "Epoch 34, iteration 5000, average loss = 2.568968\n",
      "Epoch 34, iteration 6000, average loss = 2.568301\n",
      "Epoch 34, iteration 7000, average loss = 2.567834\n",
      "Epoch 34, iteration 8000, average loss = 2.567750\n",
      "Epoch 34, iteration 9000, average loss = 2.567234\n",
      "Epoch 34, iteration 10000, average loss = 2.566557\n",
      "Epoch 34, iteration 11000, average loss = 2.566535\n",
      "Epoch 34, iteration 12000, average loss = 2.566088\n",
      "Epoch 34, iteration 13000, average loss = 2.565880\n",
      "Epoch 35, iteration 1000, average loss = 2.555867\n",
      "Epoch 35, iteration 2000, average loss = 2.555215\n",
      "Epoch 35, iteration 3000, average loss = 2.555776\n",
      "Epoch 35, iteration 4000, average loss = 2.555567\n",
      "Epoch 35, iteration 5000, average loss = 2.555749\n",
      "Epoch 35, iteration 6000, average loss = 2.555106\n",
      "Epoch 35, iteration 7000, average loss = 2.554989\n",
      "Epoch 35, iteration 8000, average loss = 2.554567\n",
      "Epoch 35, iteration 9000, average loss = 2.554604\n",
      "Epoch 35, iteration 10000, average loss = 2.554344\n",
      "Epoch 35, iteration 11000, average loss = 2.553940\n",
      "Epoch 35, iteration 12000, average loss = 2.553676\n",
      "Epoch 35, iteration 13000, average loss = 2.553156\n",
      "Epoch 36, iteration 1000, average loss = 2.543035\n",
      "Epoch 36, iteration 2000, average loss = 2.544401\n",
      "Epoch 36, iteration 3000, average loss = 2.544793\n",
      "Epoch 36, iteration 4000, average loss = 2.543826\n",
      "Epoch 36, iteration 5000, average loss = 2.544207\n",
      "Epoch 36, iteration 6000, average loss = 2.544313\n",
      "Epoch 36, iteration 7000, average loss = 2.543919\n",
      "Epoch 36, iteration 8000, average loss = 2.543605\n",
      "Epoch 36, iteration 9000, average loss = 2.543147\n",
      "Epoch 36, iteration 10000, average loss = 2.542769\n",
      "Epoch 36, iteration 11000, average loss = 2.542118\n",
      "Epoch 36, iteration 12000, average loss = 2.541713\n",
      "Epoch 36, iteration 13000, average loss = 2.541149\n",
      "Epoch 37, iteration 1000, average loss = 2.531216\n",
      "Epoch 37, iteration 2000, average loss = 2.531449\n",
      "Epoch 37, iteration 3000, average loss = 2.533536\n",
      "Epoch 37, iteration 4000, average loss = 2.533687\n",
      "Epoch 37, iteration 5000, average loss = 2.532566\n",
      "Epoch 37, iteration 6000, average loss = 2.532293\n",
      "Epoch 37, iteration 7000, average loss = 2.531791\n",
      "Epoch 37, iteration 8000, average loss = 2.531289\n",
      "Epoch 37, iteration 9000, average loss = 2.530785\n",
      "Epoch 37, iteration 10000, average loss = 2.530349\n",
      "Epoch 37, iteration 11000, average loss = 2.530046\n",
      "Epoch 37, iteration 12000, average loss = 2.529483\n",
      "Epoch 37, iteration 13000, average loss = 2.529337\n",
      "Epoch 38, iteration 1000, average loss = 2.521852\n",
      "Epoch 38, iteration 2000, average loss = 2.522247\n",
      "Epoch 38, iteration 3000, average loss = 2.522268\n",
      "Epoch 38, iteration 4000, average loss = 2.521656\n",
      "Epoch 38, iteration 5000, average loss = 2.521554\n",
      "Epoch 38, iteration 6000, average loss = 2.521251\n",
      "Epoch 38, iteration 7000, average loss = 2.520743\n",
      "Epoch 38, iteration 8000, average loss = 2.520196\n",
      "Epoch 38, iteration 9000, average loss = 2.519958\n",
      "Epoch 38, iteration 10000, average loss = 2.519901\n",
      "Epoch 38, iteration 11000, average loss = 2.519217\n",
      "Epoch 38, iteration 12000, average loss = 2.518855\n",
      "Epoch 38, iteration 13000, average loss = 2.518448\n",
      "Epoch 39, iteration 1000, average loss = 2.512415\n",
      "Epoch 39, iteration 2000, average loss = 2.510097\n",
      "Epoch 39, iteration 3000, average loss = 2.511040\n",
      "Epoch 39, iteration 4000, average loss = 2.510849\n",
      "Epoch 39, iteration 5000, average loss = 2.510515\n",
      "Epoch 39, iteration 6000, average loss = 2.510314\n",
      "Epoch 39, iteration 7000, average loss = 2.509777\n",
      "Epoch 39, iteration 8000, average loss = 2.509208\n",
      "Epoch 39, iteration 9000, average loss = 2.508744\n",
      "Epoch 39, iteration 10000, average loss = 2.508366\n",
      "Epoch 39, iteration 11000, average loss = 2.508008\n",
      "Epoch 39, iteration 12000, average loss = 2.507770\n",
      "Epoch 39, iteration 13000, average loss = 2.507399\n",
      "Epoch 40, iteration 1000, average loss = 2.501198\n",
      "Epoch 40, iteration 2000, average loss = 2.500735\n",
      "Epoch 40, iteration 3000, average loss = 2.500439\n",
      "Epoch 40, iteration 4000, average loss = 2.499928\n",
      "Epoch 40, iteration 5000, average loss = 2.499970\n",
      "Epoch 40, iteration 6000, average loss = 2.499617\n",
      "Epoch 40, iteration 7000, average loss = 2.499464\n",
      "Epoch 40, iteration 8000, average loss = 2.498913\n",
      "Epoch 40, iteration 9000, average loss = 2.498736\n",
      "Epoch 40, iteration 10000, average loss = 2.498044\n",
      "Epoch 40, iteration 11000, average loss = 2.497699\n",
      "Epoch 40, iteration 12000, average loss = 2.497368\n",
      "Epoch 40, iteration 13000, average loss = 2.497289\n",
      "Epoch 41, iteration 1000, average loss = 2.488506\n",
      "Epoch 41, iteration 2000, average loss = 2.488650\n",
      "Epoch 41, iteration 3000, average loss = 2.489055\n",
      "Epoch 41, iteration 4000, average loss = 2.489080\n",
      "Epoch 41, iteration 5000, average loss = 2.488538\n",
      "Epoch 41, iteration 6000, average loss = 2.488231\n",
      "Epoch 41, iteration 7000, average loss = 2.487899\n",
      "Epoch 41, iteration 8000, average loss = 2.488052\n",
      "Epoch 41, iteration 9000, average loss = 2.487907\n",
      "Epoch 41, iteration 10000, average loss = 2.487660\n",
      "Epoch 41, iteration 11000, average loss = 2.487234\n",
      "Epoch 41, iteration 12000, average loss = 2.487239\n",
      "Epoch 41, iteration 13000, average loss = 2.486936\n",
      "Epoch 42, iteration 1000, average loss = 2.480474\n",
      "Epoch 42, iteration 2000, average loss = 2.480929\n",
      "Epoch 42, iteration 3000, average loss = 2.481802\n",
      "Epoch 42, iteration 4000, average loss = 2.481434\n",
      "Epoch 42, iteration 5000, average loss = 2.480910\n",
      "Epoch 42, iteration 6000, average loss = 2.480236\n",
      "Epoch 42, iteration 7000, average loss = 2.479998\n",
      "Epoch 42, iteration 8000, average loss = 2.479626\n",
      "Epoch 42, iteration 9000, average loss = 2.478948\n",
      "Epoch 42, iteration 10000, average loss = 2.478257\n",
      "Epoch 42, iteration 11000, average loss = 2.478177\n",
      "Epoch 42, iteration 12000, average loss = 2.477932\n",
      "Epoch 42, iteration 13000, average loss = 2.477674\n",
      "Epoch 43, iteration 1000, average loss = 2.470457\n",
      "Epoch 43, iteration 2000, average loss = 2.471082\n",
      "Epoch 43, iteration 3000, average loss = 2.470626\n",
      "Epoch 43, iteration 4000, average loss = 2.470624\n",
      "Epoch 43, iteration 5000, average loss = 2.470269\n",
      "Epoch 43, iteration 6000, average loss = 2.469632\n",
      "Epoch 43, iteration 7000, average loss = 2.469295\n",
      "Epoch 43, iteration 8000, average loss = 2.469158\n",
      "Epoch 43, iteration 9000, average loss = 2.468947\n",
      "Epoch 43, iteration 10000, average loss = 2.468508\n",
      "Epoch 43, iteration 11000, average loss = 2.468587\n",
      "Epoch 43, iteration 12000, average loss = 2.468143\n",
      "Epoch 43, iteration 13000, average loss = 2.468002\n",
      "Epoch 44, iteration 1000, average loss = 2.460298\n",
      "Epoch 44, iteration 2000, average loss = 2.462389\n",
      "Epoch 44, iteration 3000, average loss = 2.462423\n",
      "Epoch 44, iteration 4000, average loss = 2.461773\n",
      "Epoch 44, iteration 5000, average loss = 2.461424\n",
      "Epoch 44, iteration 6000, average loss = 2.460982\n",
      "Epoch 44, iteration 7000, average loss = 2.460542\n",
      "Epoch 44, iteration 8000, average loss = 2.460379\n",
      "Epoch 44, iteration 9000, average loss = 2.459999\n",
      "Epoch 44, iteration 10000, average loss = 2.460025\n",
      "Epoch 44, iteration 11000, average loss = 2.459834\n",
      "Epoch 44, iteration 12000, average loss = 2.459646\n",
      "Epoch 44, iteration 13000, average loss = 2.459153\n",
      "Epoch 45, iteration 1000, average loss = 2.454230\n",
      "Epoch 45, iteration 2000, average loss = 2.453766\n",
      "Epoch 45, iteration 3000, average loss = 2.453466\n",
      "Epoch 45, iteration 4000, average loss = 2.452888\n",
      "Epoch 45, iteration 5000, average loss = 2.452733\n",
      "Epoch 45, iteration 6000, average loss = 2.452377\n",
      "Epoch 45, iteration 7000, average loss = 2.452198\n",
      "Epoch 45, iteration 8000, average loss = 2.451941\n",
      "Epoch 45, iteration 9000, average loss = 2.452001\n",
      "Epoch 45, iteration 10000, average loss = 2.451680\n",
      "Epoch 45, iteration 11000, average loss = 2.451355\n",
      "Epoch 45, iteration 12000, average loss = 2.451004\n",
      "Epoch 45, iteration 13000, average loss = 2.450591\n",
      "Epoch 46, iteration 1000, average loss = 2.443867\n",
      "Epoch 46, iteration 2000, average loss = 2.444939\n",
      "Epoch 46, iteration 3000, average loss = 2.444262\n",
      "Epoch 46, iteration 4000, average loss = 2.444167\n",
      "Epoch 46, iteration 5000, average loss = 2.443823\n",
      "Epoch 46, iteration 6000, average loss = 2.443611\n",
      "Epoch 46, iteration 7000, average loss = 2.443485\n",
      "Epoch 46, iteration 8000, average loss = 2.443322\n",
      "Epoch 46, iteration 9000, average loss = 2.443144\n",
      "Epoch 46, iteration 10000, average loss = 2.442757\n",
      "Epoch 46, iteration 11000, average loss = 2.442446\n",
      "Epoch 46, iteration 12000, average loss = 2.442228\n",
      "Epoch 46, iteration 13000, average loss = 2.442138\n",
      "Epoch 47, iteration 1000, average loss = 2.437199\n",
      "Epoch 47, iteration 2000, average loss = 2.436646\n",
      "Epoch 47, iteration 3000, average loss = 2.435570\n",
      "Epoch 47, iteration 4000, average loss = 2.434690\n",
      "Epoch 47, iteration 5000, average loss = 2.434757\n",
      "Epoch 47, iteration 6000, average loss = 2.434956\n",
      "Epoch 47, iteration 7000, average loss = 2.434986\n",
      "Epoch 47, iteration 8000, average loss = 2.435030\n",
      "Epoch 47, iteration 9000, average loss = 2.434963\n",
      "Epoch 47, iteration 10000, average loss = 2.434761\n",
      "Epoch 47, iteration 11000, average loss = 2.434416\n",
      "Epoch 47, iteration 12000, average loss = 2.434350\n",
      "Epoch 47, iteration 13000, average loss = 2.434198\n",
      "Epoch 48, iteration 1000, average loss = 2.429380\n",
      "Epoch 48, iteration 2000, average loss = 2.428023\n",
      "Epoch 48, iteration 3000, average loss = 2.427862\n",
      "Epoch 48, iteration 4000, average loss = 2.428394\n",
      "Epoch 48, iteration 5000, average loss = 2.428706\n",
      "Epoch 48, iteration 6000, average loss = 2.428321\n",
      "Epoch 48, iteration 7000, average loss = 2.427935\n",
      "Epoch 48, iteration 8000, average loss = 2.427751\n",
      "Epoch 48, iteration 9000, average loss = 2.427507\n",
      "Epoch 48, iteration 10000, average loss = 2.427508\n",
      "Epoch 48, iteration 11000, average loss = 2.426963\n",
      "Epoch 48, iteration 12000, average loss = 2.426638\n",
      "Epoch 48, iteration 13000, average loss = 2.426154\n",
      "Epoch 49, iteration 1000, average loss = 2.422101\n",
      "Epoch 49, iteration 2000, average loss = 2.420963\n",
      "Epoch 49, iteration 3000, average loss = 2.421160\n",
      "Epoch 49, iteration 4000, average loss = 2.420722\n",
      "Epoch 49, iteration 5000, average loss = 2.420572\n",
      "Epoch 49, iteration 6000, average loss = 2.419994\n",
      "Epoch 49, iteration 7000, average loss = 2.420057\n",
      "Epoch 49, iteration 8000, average loss = 2.419766\n",
      "Epoch 49, iteration 9000, average loss = 2.419093\n",
      "Epoch 49, iteration 10000, average loss = 2.418977\n",
      "Epoch 49, iteration 11000, average loss = 2.418505\n",
      "Epoch 49, iteration 12000, average loss = 2.418547\n",
      "Epoch 49, iteration 13000, average loss = 2.418241\n",
      "Epoch 50, iteration 1000, average loss = 2.412348\n",
      "Epoch 50, iteration 2000, average loss = 2.412473\n",
      "Epoch 50, iteration 3000, average loss = 2.412712\n",
      "Epoch 50, iteration 4000, average loss = 2.412675\n",
      "Epoch 50, iteration 5000, average loss = 2.412391\n",
      "Epoch 50, iteration 6000, average loss = 2.412258\n",
      "Epoch 50, iteration 7000, average loss = 2.412408\n",
      "Epoch 50, iteration 8000, average loss = 2.412132\n",
      "Epoch 50, iteration 9000, average loss = 2.411987\n",
      "Epoch 50, iteration 10000, average loss = 2.411673\n",
      "Epoch 50, iteration 11000, average loss = 2.411603\n",
      "Epoch 50, iteration 12000, average loss = 2.411198\n",
      "Epoch 50, iteration 13000, average loss = 2.411019\n"
     ]
    }
   ],
   "source": [
    "# Обучение занимает около 3-4 суток\n",
    "'''\n",
    "model.train()\n",
    "n_epoch = 50\n",
    "\n",
    "for epoch in range(1, n_epoch+1):\n",
    "    t_loss = 0\n",
    "    sh, sc = model.init_hidden(batch_size)\n",
    "    for i, (x, y) in enumerate(loader):    \n",
    "        if x.shape[0] == batch_size:\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, (sh,sc) = model(inputs, (sh, sc))\n",
    "            loss = loss_func(output.transpose(1, 2), targets)\n",
    "            sh, sc= sh.detach(), sc.detach()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            t_loss += loss.item()\n",
    "        if (i+1)%1000 == 0:\n",
    "            print_loss = t_loss / (i+1)\n",
    "            print(f'Epoch {epoch}, iteration {i+1}, average loss = {print_loss:.6f}')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16c6b472-c6da-4f9b-a9ce-0767f845a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Trained Model\n",
    "dir_name = 'files'\n",
    "model_file_name = 'wordLSTM.pth'\n",
    "dict_file_name = 'word_to_int.p'\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), f'{dir_name}{sep}{model_file_name}')\n",
    "with open(f'{dir_name}{sep}{dict_file_name}', 'wb') as fb:    \n",
    "    pickle.dump(word_to_int, fb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b303f3c-f80d-4eb5-8a44-275c54a133ef",
   "metadata": {},
   "source": [
    "# 5 Generate text with the trained LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac04d27d-0f7f-4ed6-8bc8-5700ed1fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284becfb-ad36-44ae-b98d-708b52a50a8e",
   "metadata": {},
   "source": [
    "## 5.1 Generate text by predicting the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecd1648b-b843-490e-ad79-96cfdcc8696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{dir_name}{sep}{model_file_name}', weights_only=True))\n",
    "\n",
    "with open(f'{dir_name}{sep}{dict_file_name}', 'rb') as fb:    \n",
    "    word_to_int = pickle.load(fb)\n",
    "\n",
    "int_to_word = {v: k for k, v in word_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ef6910-d99b-45a5-9f08-1200128a3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, prompt, length=200):\n",
    "    model.eval()\n",
    "    text = prompt.lower().split(' ')\n",
    "    hc = model.init_hidden(1)\n",
    "    # Количество токенов, которые необходимо сгенерировать\n",
    "    length = length - len(text)\n",
    "\n",
    "    for i in range(0, length):\n",
    "        # При генерации следующего токена учитываем длину текущей последовательности.\n",
    "        # Если она меньше 100 токенов, мы вводим всю последовательность в модель;\n",
    "        # если она больше 100 токенов, в качестве входных данных используются\n",
    "        # только последние 100 токенов последовательности.\n",
    "        if len(text) <= seq_len:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
    "        else:\n",
    "            # Входные данные — текущая последовательность;\n",
    "            # обрезаем ее, если она длиннее 100 токенов.\n",
    "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])            \n",
    "    \n",
    "        inputs = x.to(device)\n",
    "        output, hc = model(inputs, hc)\n",
    "        logits = output[0][-1]\n",
    "\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        # Первый параметр метода указывает диапазон выбора, который в данном случае равен len(logits) = 12778.\n",
    "        # Это означает, что модель будет случайным образом выбирать целое число от 0 до 12777,\n",
    "        # причем каждое целое число будет соответствовать разному токену в словаре.\n",
    "        # Второй параметр, p, представляет собой массив, содержащий 12778 элементов,\n",
    "        # где каждый элемент обозначает вероятность выбора соответствующего токена из словаря.\n",
    "        # Токены с более высокой вероятностью в этом массиве будут выбраны с большей вероятностью.\n",
    "        idx = np.random.choice(len(logits), p=p)\n",
    "        text.append(int_to_word[idx])\n",
    "\n",
    "    text = ' '.join(text)\n",
    "    for m in \",.:;?!$()/_&%*@'`\":\n",
    "        text = text.replace(f' {m}', f'{m} ')\n",
    "    \n",
    "    text = text.replace('\"  ', '\"')   \n",
    "    text = text.replace(\"'  \", \"'\")  \n",
    "    text=text.replace('\" ', '\"')   \n",
    "    text=text.replace(\"' \", \"'\")     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab21553a-4943-4b80-8c7c-db2186fffe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince went across kitty to have to a lump.  anna had born waiting at the first day.  but that doctor was being brought on them,  and each one after another.  early paces away,  and then go so far to katavasov,  or it was only to cover home along every day again,  and went out to the pavilion to the elegant people where the muffled were come,  but come to ask her there to stand.  \"\"so i tell you if you have,  we can go on talking about the last time.  \"\"ah,  what are you going to have mercy over?  it's ten years given money,  \"he said to veslovsky,  talking to his brother and speaking at once being very tiresome if he said a great deal.  \"and i imagine that that i maintain her so far to me.  i am being himself alone,  and not cold with his mother.  he had certainly see everything the evening,  while i had told me i am but a wicked science creature,  and\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "gen_text = sample(model, prompt='Anna and the prince')\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1f2d5-6433-4d8a-9257-23a14185195d",
   "metadata": {},
   "source": [
    "## 5.2 Temperature and top-K sampling in text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00750fa9-86ab-46bb-8290-b1cd74bdeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Более низкая temperature (ниже 1; например, 0,8) приводит к меньшему количеству вариаций,\n",
    "# делая модель более детерминированной и консервативной, благоприятствуя более вероятным выборам.\n",
    "# И наоборот, более высокая temperature (выше 1; например, 1,5) повышает вероятность выбора\n",
    "# маловероятных слов при генерации текста.\n",
    "#\n",
    "# Выборка top_K — еще один метод влияния на выходные данные. Этот подход подразумевает выбор следующего слова\n",
    "# из K самых вероятных вариантов, предсказанных моделью. Распределение вероятностей усекается, чтобы включить\n",
    "# только K самых вероятных слов. При небольшом значении K, например 5, выбор модели ограничен несколькими\n",
    "# высоковероятными словами, что приводит к более предсказуемым и связным,\n",
    "# но потенциально менее разнообразным и интересным выходным данным.\n",
    "def generate(model, prompt, top_k=None, length=200, temperature=1):\n",
    "    model.eval()\n",
    "    text = prompt.lower().split(' ')\n",
    "    hc = model.init_hidden(1)\n",
    "    length = length - len(text)    \n",
    "    \n",
    "    for i in range(0, length):\n",
    "        if len(text) <= seq_len:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text]])\n",
    "        else:\n",
    "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])    \n",
    "        \n",
    "        inputs = x.to(device)\n",
    "        output, hc = model(inputs, hc)\n",
    "        logits = output[0][-1]\n",
    "        \n",
    "        # scale the logits with the temperature \n",
    "        logits = logits/temperature\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu()    \n",
    "        \n",
    "        if top_k is None:\n",
    "            idx = np.random.choice(len(logits), p=p.numpy())\n",
    "        else:\n",
    "            # Оставить только top_K наиболее вероятных кандидатов\n",
    "            ps, tops = p.topk(top_k)\n",
    "            ps = ps / ps.sum()\n",
    "            # Выбирать следующий токен из top_K кандидатов\n",
    "            idx = np.random.choice(tops, p=ps.numpy())          \n",
    "        text.append(int_to_word[idx])\n",
    "    \n",
    "    text = ' '.join(text)\n",
    "    for m in \",.:;?!$()/_&%*@'`\":\n",
    "        text = text.replace(f' {m}', f'{m} ')\n",
    "    \n",
    "    text = text.replace('\"  ', '\"')   \n",
    "    text = text.replace(\"'  \", \"'\")  \n",
    "    text = text.replace('\" ', '\"')   \n",
    "    text = text.replace(\"' \", \"'\")     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f31928aa-8be2-44ac-bbaf-6bf9177521f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm not going to see you\n",
      "i'm not going to see myself\n",
      "i'm not going to see him\n",
      "i'm not going to see you\n",
      "i'm not going to see it\n",
      "i'm not going to see it\n",
      "i'm not going to see her\n",
      "i'm not going to see your\n",
      "i'm not going to see you\n",
      "i'm not going to see him\n"
     ]
    }
   ],
   "source": [
    "# Следующий токен с использованием настроек по умолчанию\n",
    "prompt = \"I ' m not going to see\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(10):\n",
    "    # Добавить к подсказке только один дополнительный токен\n",
    "    gen_text = generate(model, prompt, top_k=None, length=len(prompt.split(' '))+1, temperature=1)\n",
    "    print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d6bac31-142b-4d00-a5e3-0b20ef9bc07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm not going to see you\n",
      "i'm not going to see her\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n"
     ]
    }
   ],
   "source": [
    "# Следующий токен с использованием консервативных прогнозов\n",
    "prompt = \"I ' m not going to see\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(10):\n",
    "    # Добавить к подсказке только один дополнительный токен\n",
    "    gen_text = generate(model, prompt, top_k=3, length=len(prompt.split(' '))+1, temperature=0.5)\n",
    "    print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aded307f-485e-47d5-aa3d-e6aefea0055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince went to the door.  she was silent,  and she began walking up to her head,  and with rapid steps she went to her husband.  \"i am very glad to tell you that.  \"\"i'm not going to say,  \"said kitty,  smiling,  and holding her up,  she went out to dress,  and began walking up and down.  \"what do you know?  \"\"yes,  i'm not going to say good bye,  \"said stepan arkadyevitch,  getting up.  \"well,  what is it?  \"\"i've not seen you long ago,  \"said stepan arkadyevitch,  smiling.  \"well,  what do you say?  \"\"oh,  no,  but how can you say that?  \"said levin.  \"oh,  yes!  \"said levin,  smiling.  \"oh,  yes,  sir.  \"\"well,  what do you think?  \"\"oh,  i shall have known you.  \"\"yes,  but how is it you\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "gen_text = generate(model, prompt='Anna and the prince', top_k=3, temperature=0.5)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7abc85-4597-4466-87a3-2e859c3da502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dgai)",
   "language": "python",
   "name": "dgai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
